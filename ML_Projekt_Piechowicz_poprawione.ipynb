{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgdQaYM1zBJtDL3luErn0M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarylaP/R.liniowa/blob/main/ML_Projekt_Piechowicz_poprawione.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.ensemble import RandomForestRegressor, AdaBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import svm\n",
        "from mlxtend.plotting import plot_decision_regions\n",
        "import sklearn.preprocessing\n",
        "\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.utils import resample\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n"
      ],
      "metadata": {
        "id": "mdQkCkpsgJ4I"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 254,
      "metadata": {
        "id": "5zdjf1EXfyhS"
      },
      "outputs": [],
      "source": [
        "def read_files(filelist):\n",
        "  result = []\n",
        "  for name in filelist:\n",
        "    with open(name) as data:\n",
        "        lines = data.readlines()\n",
        "        l = [x[:-1] for x in lines]\n",
        "        del l[::2]\n",
        "    result.extend(l)\n",
        "  return result\n",
        "\n",
        "test_lines_neg = read_files(['test_neg-K.fsa','test_neg-P.fsa','test_neg-R.fsa','test_neg-T.fsa'])\n",
        "test_lines_pos = read_files(['test_pos-K.fsa','test_pos-P.fsa','test_pos-R.fsa','test_pos-T.fsa'])\n",
        "train_lines_pos = read_files(['train_pos-K.fsa','train_pos-P.fsa','train_pos-R.fsa','train_pos-T.fsa'])\n",
        "train_lines_neg = read_files(['train_neg-K.fsa','train_neg-P.fsa','train_neg-R.fsa','train_neg-T.fsa'])\n",
        "\n",
        "# test_lines_neg = read_files(['test_neg-P.fsa'])\n",
        "# test_lines_pos = read_files(['test_pos-P.fsa'])\n",
        "# train_lines_pos = read_files(['train_pos-P.fsa'])\n",
        "# train_lines_neg = read_files(['train_neg-P.fsa'])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_lines_pos=train_lines_pos*6"
      ],
      "metadata": {
        "id": "qm8JQY555kFn"
      },
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_lines_neg))\n",
        "print(len(train_lines_pos))\n",
        "len(train_lines_neg)/len(train_lines_pos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSjJhxzE3TRR",
        "outputId": "5e8086b1-a724-42e4-d405-84a935455c76"
      },
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3974\n",
            "3690\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0769647696476965"
            ]
          },
          "metadata": {},
          "execution_count": 256
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('nine_physicochemical_properties_of_amino_acid_original and corresponding molecular weight.csv', delimiter= ';', decimal = ',').set_index(\"Amino Acid\")\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 769
        },
        "id": "3dG5F3rci5B2",
        "outputId": "4c896728-8615-4161-e109-bd00717d42b6"
      },
      "execution_count": 257,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            Hydrophobicity  Hydrophilicity  Mass   pK1    pK2     pI  \\\n",
              "Amino Acid                                                             \n",
              "A                     0.62            -0.5    15  2.35   9.87   6.11   \n",
              "V                     0.29            -1.0    47  1.71  10.78   5.02   \n",
              "L                    -0.90             3.0    59  1.88   9.60   2.98   \n",
              "I                    -0.74             3.0    73  2.19   9.67   3.08   \n",
              "P                     1.19            -2.5    91  2.58   9.24   5.91   \n",
              "F                     0.48             0.0     1  2.34   9.60   6.06   \n",
              "W                    -0.40            -0.5    82  1.78   8.97   7.64   \n",
              "M                     1.38            -1.8    57  2.32   9.76   6.04   \n",
              "G                    -1.50             3.0    73  2.20   8.90   9.47   \n",
              "S                     1.06            -1.8    57  2.36   9.60   6.04   \n",
              "T                     0.64            -1.3    75  2.28   9.21   5.74   \n",
              "C                    -0.78             0.2    58  2.18   9.09  10.76   \n",
              "Y                     0.12             0.0    42  1.99  10.06   6.30   \n",
              "N                    -0.85             0.2    72  2.17   9.13   5.65   \n",
              "Q                    -2.53             3.0   101  2.18   9.09  10.76   \n",
              "K                    -0.18             0.3    31  2.21   9.15   5.68   \n",
              "R                    -0.05            -0.4    45  2.15   9.12   5.60   \n",
              "H                     1.08            -1.5    43  2.29   9.74   6.02   \n",
              "D                     0.81            -3.4   130  2.38   9.39   5.88   \n",
              "E                     0.26            -2.3   107  2.20   9.11   5.63   \n",
              "\n",
              "            Rigidity  Flexibility  Irreplaceability  \n",
              "Amino Acid                                           \n",
              "A              -1.34        -3.10              0.52  \n",
              "V              -1.51         0.96              1.12  \n",
              "L              -0.20         0.42              0.77  \n",
              "I              -0.37         2.01              0.76  \n",
              "P               2.88        -0.45              0.86  \n",
              "F              -1.10        -2.73              0.56  \n",
              "W               2.27        -0.22              0.94  \n",
              "M              -1.74         0.42              0.65  \n",
              "G              -1.82         3.95              0.81  \n",
              "S              -1.74         0.42              0.58  \n",
              "T              -1.74         2.48              1.25  \n",
              "C              -0.20         0.42              0.79  \n",
              "Y               1.98        -2.40              0.61  \n",
              "N              -0.35         2.01              0.86  \n",
              "Q               1.17         3.06              0.60  \n",
              "K              -1.51         0.96              0.64  \n",
              "R              -1.64        -1.32              0.56  \n",
              "H              -1.64        -1.32              0.54  \n",
              "D               5.91        -1.00              1.82  \n",
              "E               2.71        -0.67              0.98  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9c8ae93a-3419-4598-b9f0-44d698dbbeb6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Hydrophobicity</th>\n",
              "      <th>Hydrophilicity</th>\n",
              "      <th>Mass</th>\n",
              "      <th>pK1</th>\n",
              "      <th>pK2</th>\n",
              "      <th>pI</th>\n",
              "      <th>Rigidity</th>\n",
              "      <th>Flexibility</th>\n",
              "      <th>Irreplaceability</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Amino Acid</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>A</th>\n",
              "      <td>0.62</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>15</td>\n",
              "      <td>2.35</td>\n",
              "      <td>9.87</td>\n",
              "      <td>6.11</td>\n",
              "      <td>-1.34</td>\n",
              "      <td>-3.10</td>\n",
              "      <td>0.52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V</th>\n",
              "      <td>0.29</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>47</td>\n",
              "      <td>1.71</td>\n",
              "      <td>10.78</td>\n",
              "      <td>5.02</td>\n",
              "      <td>-1.51</td>\n",
              "      <td>0.96</td>\n",
              "      <td>1.12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>L</th>\n",
              "      <td>-0.90</td>\n",
              "      <td>3.0</td>\n",
              "      <td>59</td>\n",
              "      <td>1.88</td>\n",
              "      <td>9.60</td>\n",
              "      <td>2.98</td>\n",
              "      <td>-0.20</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>I</th>\n",
              "      <td>-0.74</td>\n",
              "      <td>3.0</td>\n",
              "      <td>73</td>\n",
              "      <td>2.19</td>\n",
              "      <td>9.67</td>\n",
              "      <td>3.08</td>\n",
              "      <td>-0.37</td>\n",
              "      <td>2.01</td>\n",
              "      <td>0.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>P</th>\n",
              "      <td>1.19</td>\n",
              "      <td>-2.5</td>\n",
              "      <td>91</td>\n",
              "      <td>2.58</td>\n",
              "      <td>9.24</td>\n",
              "      <td>5.91</td>\n",
              "      <td>2.88</td>\n",
              "      <td>-0.45</td>\n",
              "      <td>0.86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>F</th>\n",
              "      <td>0.48</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.34</td>\n",
              "      <td>9.60</td>\n",
              "      <td>6.06</td>\n",
              "      <td>-1.10</td>\n",
              "      <td>-2.73</td>\n",
              "      <td>0.56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>W</th>\n",
              "      <td>-0.40</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>82</td>\n",
              "      <td>1.78</td>\n",
              "      <td>8.97</td>\n",
              "      <td>7.64</td>\n",
              "      <td>2.27</td>\n",
              "      <td>-0.22</td>\n",
              "      <td>0.94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>M</th>\n",
              "      <td>1.38</td>\n",
              "      <td>-1.8</td>\n",
              "      <td>57</td>\n",
              "      <td>2.32</td>\n",
              "      <td>9.76</td>\n",
              "      <td>6.04</td>\n",
              "      <td>-1.74</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>G</th>\n",
              "      <td>-1.50</td>\n",
              "      <td>3.0</td>\n",
              "      <td>73</td>\n",
              "      <td>2.20</td>\n",
              "      <td>8.90</td>\n",
              "      <td>9.47</td>\n",
              "      <td>-1.82</td>\n",
              "      <td>3.95</td>\n",
              "      <td>0.81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>S</th>\n",
              "      <td>1.06</td>\n",
              "      <td>-1.8</td>\n",
              "      <td>57</td>\n",
              "      <td>2.36</td>\n",
              "      <td>9.60</td>\n",
              "      <td>6.04</td>\n",
              "      <td>-1.74</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T</th>\n",
              "      <td>0.64</td>\n",
              "      <td>-1.3</td>\n",
              "      <td>75</td>\n",
              "      <td>2.28</td>\n",
              "      <td>9.21</td>\n",
              "      <td>5.74</td>\n",
              "      <td>-1.74</td>\n",
              "      <td>2.48</td>\n",
              "      <td>1.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C</th>\n",
              "      <td>-0.78</td>\n",
              "      <td>0.2</td>\n",
              "      <td>58</td>\n",
              "      <td>2.18</td>\n",
              "      <td>9.09</td>\n",
              "      <td>10.76</td>\n",
              "      <td>-0.20</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Y</th>\n",
              "      <td>0.12</td>\n",
              "      <td>0.0</td>\n",
              "      <td>42</td>\n",
              "      <td>1.99</td>\n",
              "      <td>10.06</td>\n",
              "      <td>6.30</td>\n",
              "      <td>1.98</td>\n",
              "      <td>-2.40</td>\n",
              "      <td>0.61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>N</th>\n",
              "      <td>-0.85</td>\n",
              "      <td>0.2</td>\n",
              "      <td>72</td>\n",
              "      <td>2.17</td>\n",
              "      <td>9.13</td>\n",
              "      <td>5.65</td>\n",
              "      <td>-0.35</td>\n",
              "      <td>2.01</td>\n",
              "      <td>0.86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Q</th>\n",
              "      <td>-2.53</td>\n",
              "      <td>3.0</td>\n",
              "      <td>101</td>\n",
              "      <td>2.18</td>\n",
              "      <td>9.09</td>\n",
              "      <td>10.76</td>\n",
              "      <td>1.17</td>\n",
              "      <td>3.06</td>\n",
              "      <td>0.60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>K</th>\n",
              "      <td>-0.18</td>\n",
              "      <td>0.3</td>\n",
              "      <td>31</td>\n",
              "      <td>2.21</td>\n",
              "      <td>9.15</td>\n",
              "      <td>5.68</td>\n",
              "      <td>-1.51</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0.64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>R</th>\n",
              "      <td>-0.05</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>45</td>\n",
              "      <td>2.15</td>\n",
              "      <td>9.12</td>\n",
              "      <td>5.60</td>\n",
              "      <td>-1.64</td>\n",
              "      <td>-1.32</td>\n",
              "      <td>0.56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>H</th>\n",
              "      <td>1.08</td>\n",
              "      <td>-1.5</td>\n",
              "      <td>43</td>\n",
              "      <td>2.29</td>\n",
              "      <td>9.74</td>\n",
              "      <td>6.02</td>\n",
              "      <td>-1.64</td>\n",
              "      <td>-1.32</td>\n",
              "      <td>0.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D</th>\n",
              "      <td>0.81</td>\n",
              "      <td>-3.4</td>\n",
              "      <td>130</td>\n",
              "      <td>2.38</td>\n",
              "      <td>9.39</td>\n",
              "      <td>5.88</td>\n",
              "      <td>5.91</td>\n",
              "      <td>-1.00</td>\n",
              "      <td>1.82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>E</th>\n",
              "      <td>0.26</td>\n",
              "      <td>-2.3</td>\n",
              "      <td>107</td>\n",
              "      <td>2.20</td>\n",
              "      <td>9.11</td>\n",
              "      <td>5.63</td>\n",
              "      <td>2.71</td>\n",
              "      <td>-0.67</td>\n",
              "      <td>0.98</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9c8ae93a-3419-4598-b9f0-44d698dbbeb6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9c8ae93a-3419-4598-b9f0-44d698dbbeb6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9c8ae93a-3419-4598-b9f0-44d698dbbeb6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 257
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datast = (data-data.mean())/data.std()"
      ],
      "metadata": {
        "id": "drDoFOY0f0A0"
      },
      "execution_count": 258,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datast = datast.append(pd.DataFrame([[0]*9], columns=datast.columns, index=['X']))"
      ],
      "metadata": {
        "id": "Pu5ewJtA0OPq"
      },
      "execution_count": 259,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datast"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 772
        },
        "id": "MZE8UZ_r1MHN",
        "outputId": "2640c0be-0542-4545-de86-5f8444bfa09b"
      },
      "execution_count": 260,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Hydrophobicity  Hydrophilicity      Mass       pK1       pK2        pI  \\\n",
              "A        0.620140       -0.148032 -1.551628  0.776413  0.914983 -0.103339   \n",
              "V        0.290066       -0.407738 -0.516131 -2.272079  2.916509 -0.643579   \n",
              "L       -0.900204        1.669909 -0.127819 -1.462324  0.321124 -1.654670   \n",
              "I       -0.740168        1.669909  0.325211  0.014290  0.475087 -1.605107   \n",
              "P        1.190269       -1.186856  0.907678  1.871965 -0.470688 -0.202466   \n",
              "F        0.480109        0.111674 -2.004658  0.728780  0.321124 -0.128121   \n",
              "W       -0.400091       -0.148032  0.616444 -1.938651 -1.064548  0.654979   \n",
              "M        1.380312       -0.823268 -0.192538  0.633515  0.673041 -0.138034   \n",
              "G       -1.500340        1.669909  0.325211  0.061923 -1.218511  1.561986   \n",
              "S        1.060240       -0.823268 -0.192538  0.824046  0.321124 -0.138034   \n",
              "T        0.640145       -0.563562  0.389929  0.442984 -0.536673 -0.286724   \n",
              "C       -0.780177        0.215556 -0.160178 -0.033343 -0.800610  2.201353   \n",
              "Y        0.120027        0.111674 -0.677927 -0.938364  1.332884 -0.009169   \n",
              "N       -0.850192        0.215556  0.292852 -0.080976 -0.712631 -0.331330   \n",
              "Q       -2.530573        1.669909  1.231271 -0.033343 -0.800610  2.201353   \n",
              "K       -0.180041        0.267497 -1.033879  0.109555 -0.668642 -0.316461   \n",
              "R       -0.050011       -0.096091 -0.580849 -0.176241 -0.734626 -0.356112   \n",
              "H        1.080245       -0.667444 -0.645568  0.490617  0.629051 -0.147946   \n",
              "D        0.810183       -1.654327  2.169690  0.919311 -0.140767 -0.217335   \n",
              "E        0.260059       -1.082974  1.425426  0.061923 -0.756621 -0.341243   \n",
              "X        0.000000        0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "\n",
              "   Rigidity  Flexibility  Irreplaceability  \n",
              "A -0.627806    -1.729144         -0.935467  \n",
              "V -0.707393     0.401455          0.993331  \n",
              "L -0.094101     0.118075         -0.131801  \n",
              "I -0.173688     0.952473         -0.163948  \n",
              "P  1.347839    -0.338482          0.157519  \n",
              "F -0.515447    -1.534976         -0.806881  \n",
              "W  1.062260    -0.217783          0.414692  \n",
              "M -0.815070     0.118075         -0.517561  \n",
              "G -0.852523     1.970542         -0.003215  \n",
              "S -0.815070     0.118075         -0.742587  \n",
              "T -0.815070     1.199118          1.411237  \n",
              "C -0.094101     0.118075         -0.067508  \n",
              "Y  0.926493    -1.361800         -0.646147  \n",
              "N -0.164325     0.952473          0.157519  \n",
              "Q  0.547282     1.503490         -0.678294  \n",
              "K -0.707393     0.401455         -0.549707  \n",
              "R -0.768254    -0.795039         -0.806881  \n",
              "H -0.768254    -0.795039         -0.871174  \n",
              "D  2.766371    -0.627110          3.243596  \n",
              "E  1.268251    -0.453933          0.543278  \n",
              "X  0.000000     0.000000          0.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-643067ce-607f-42a7-8fe1-a35916db954c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Hydrophobicity</th>\n",
              "      <th>Hydrophilicity</th>\n",
              "      <th>Mass</th>\n",
              "      <th>pK1</th>\n",
              "      <th>pK2</th>\n",
              "      <th>pI</th>\n",
              "      <th>Rigidity</th>\n",
              "      <th>Flexibility</th>\n",
              "      <th>Irreplaceability</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>A</th>\n",
              "      <td>0.620140</td>\n",
              "      <td>-0.148032</td>\n",
              "      <td>-1.551628</td>\n",
              "      <td>0.776413</td>\n",
              "      <td>0.914983</td>\n",
              "      <td>-0.103339</td>\n",
              "      <td>-0.627806</td>\n",
              "      <td>-1.729144</td>\n",
              "      <td>-0.935467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V</th>\n",
              "      <td>0.290066</td>\n",
              "      <td>-0.407738</td>\n",
              "      <td>-0.516131</td>\n",
              "      <td>-2.272079</td>\n",
              "      <td>2.916509</td>\n",
              "      <td>-0.643579</td>\n",
              "      <td>-0.707393</td>\n",
              "      <td>0.401455</td>\n",
              "      <td>0.993331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>L</th>\n",
              "      <td>-0.900204</td>\n",
              "      <td>1.669909</td>\n",
              "      <td>-0.127819</td>\n",
              "      <td>-1.462324</td>\n",
              "      <td>0.321124</td>\n",
              "      <td>-1.654670</td>\n",
              "      <td>-0.094101</td>\n",
              "      <td>0.118075</td>\n",
              "      <td>-0.131801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>I</th>\n",
              "      <td>-0.740168</td>\n",
              "      <td>1.669909</td>\n",
              "      <td>0.325211</td>\n",
              "      <td>0.014290</td>\n",
              "      <td>0.475087</td>\n",
              "      <td>-1.605107</td>\n",
              "      <td>-0.173688</td>\n",
              "      <td>0.952473</td>\n",
              "      <td>-0.163948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>P</th>\n",
              "      <td>1.190269</td>\n",
              "      <td>-1.186856</td>\n",
              "      <td>0.907678</td>\n",
              "      <td>1.871965</td>\n",
              "      <td>-0.470688</td>\n",
              "      <td>-0.202466</td>\n",
              "      <td>1.347839</td>\n",
              "      <td>-0.338482</td>\n",
              "      <td>0.157519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>F</th>\n",
              "      <td>0.480109</td>\n",
              "      <td>0.111674</td>\n",
              "      <td>-2.004658</td>\n",
              "      <td>0.728780</td>\n",
              "      <td>0.321124</td>\n",
              "      <td>-0.128121</td>\n",
              "      <td>-0.515447</td>\n",
              "      <td>-1.534976</td>\n",
              "      <td>-0.806881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>W</th>\n",
              "      <td>-0.400091</td>\n",
              "      <td>-0.148032</td>\n",
              "      <td>0.616444</td>\n",
              "      <td>-1.938651</td>\n",
              "      <td>-1.064548</td>\n",
              "      <td>0.654979</td>\n",
              "      <td>1.062260</td>\n",
              "      <td>-0.217783</td>\n",
              "      <td>0.414692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>M</th>\n",
              "      <td>1.380312</td>\n",
              "      <td>-0.823268</td>\n",
              "      <td>-0.192538</td>\n",
              "      <td>0.633515</td>\n",
              "      <td>0.673041</td>\n",
              "      <td>-0.138034</td>\n",
              "      <td>-0.815070</td>\n",
              "      <td>0.118075</td>\n",
              "      <td>-0.517561</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>G</th>\n",
              "      <td>-1.500340</td>\n",
              "      <td>1.669909</td>\n",
              "      <td>0.325211</td>\n",
              "      <td>0.061923</td>\n",
              "      <td>-1.218511</td>\n",
              "      <td>1.561986</td>\n",
              "      <td>-0.852523</td>\n",
              "      <td>1.970542</td>\n",
              "      <td>-0.003215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>S</th>\n",
              "      <td>1.060240</td>\n",
              "      <td>-0.823268</td>\n",
              "      <td>-0.192538</td>\n",
              "      <td>0.824046</td>\n",
              "      <td>0.321124</td>\n",
              "      <td>-0.138034</td>\n",
              "      <td>-0.815070</td>\n",
              "      <td>0.118075</td>\n",
              "      <td>-0.742587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T</th>\n",
              "      <td>0.640145</td>\n",
              "      <td>-0.563562</td>\n",
              "      <td>0.389929</td>\n",
              "      <td>0.442984</td>\n",
              "      <td>-0.536673</td>\n",
              "      <td>-0.286724</td>\n",
              "      <td>-0.815070</td>\n",
              "      <td>1.199118</td>\n",
              "      <td>1.411237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C</th>\n",
              "      <td>-0.780177</td>\n",
              "      <td>0.215556</td>\n",
              "      <td>-0.160178</td>\n",
              "      <td>-0.033343</td>\n",
              "      <td>-0.800610</td>\n",
              "      <td>2.201353</td>\n",
              "      <td>-0.094101</td>\n",
              "      <td>0.118075</td>\n",
              "      <td>-0.067508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Y</th>\n",
              "      <td>0.120027</td>\n",
              "      <td>0.111674</td>\n",
              "      <td>-0.677927</td>\n",
              "      <td>-0.938364</td>\n",
              "      <td>1.332884</td>\n",
              "      <td>-0.009169</td>\n",
              "      <td>0.926493</td>\n",
              "      <td>-1.361800</td>\n",
              "      <td>-0.646147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>N</th>\n",
              "      <td>-0.850192</td>\n",
              "      <td>0.215556</td>\n",
              "      <td>0.292852</td>\n",
              "      <td>-0.080976</td>\n",
              "      <td>-0.712631</td>\n",
              "      <td>-0.331330</td>\n",
              "      <td>-0.164325</td>\n",
              "      <td>0.952473</td>\n",
              "      <td>0.157519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Q</th>\n",
              "      <td>-2.530573</td>\n",
              "      <td>1.669909</td>\n",
              "      <td>1.231271</td>\n",
              "      <td>-0.033343</td>\n",
              "      <td>-0.800610</td>\n",
              "      <td>2.201353</td>\n",
              "      <td>0.547282</td>\n",
              "      <td>1.503490</td>\n",
              "      <td>-0.678294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>K</th>\n",
              "      <td>-0.180041</td>\n",
              "      <td>0.267497</td>\n",
              "      <td>-1.033879</td>\n",
              "      <td>0.109555</td>\n",
              "      <td>-0.668642</td>\n",
              "      <td>-0.316461</td>\n",
              "      <td>-0.707393</td>\n",
              "      <td>0.401455</td>\n",
              "      <td>-0.549707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>R</th>\n",
              "      <td>-0.050011</td>\n",
              "      <td>-0.096091</td>\n",
              "      <td>-0.580849</td>\n",
              "      <td>-0.176241</td>\n",
              "      <td>-0.734626</td>\n",
              "      <td>-0.356112</td>\n",
              "      <td>-0.768254</td>\n",
              "      <td>-0.795039</td>\n",
              "      <td>-0.806881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>H</th>\n",
              "      <td>1.080245</td>\n",
              "      <td>-0.667444</td>\n",
              "      <td>-0.645568</td>\n",
              "      <td>0.490617</td>\n",
              "      <td>0.629051</td>\n",
              "      <td>-0.147946</td>\n",
              "      <td>-0.768254</td>\n",
              "      <td>-0.795039</td>\n",
              "      <td>-0.871174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D</th>\n",
              "      <td>0.810183</td>\n",
              "      <td>-1.654327</td>\n",
              "      <td>2.169690</td>\n",
              "      <td>0.919311</td>\n",
              "      <td>-0.140767</td>\n",
              "      <td>-0.217335</td>\n",
              "      <td>2.766371</td>\n",
              "      <td>-0.627110</td>\n",
              "      <td>3.243596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>E</th>\n",
              "      <td>0.260059</td>\n",
              "      <td>-1.082974</td>\n",
              "      <td>1.425426</td>\n",
              "      <td>0.061923</td>\n",
              "      <td>-0.756621</td>\n",
              "      <td>-0.341243</td>\n",
              "      <td>1.268251</td>\n",
              "      <td>-0.453933</td>\n",
              "      <td>0.543278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-643067ce-607f-42a7-8fe1-a35916db954c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-643067ce-607f-42a7-8fe1-a35916db954c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-643067ce-607f-42a7-8fe1-a35916db954c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 260
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def seq_to_feature_vec(input):\n",
        "  sample = []\n",
        "  for a in input:\n",
        "    sample.append(datast.loc[a].to_numpy())\n",
        "  sample = np.concatenate(sample)\n",
        "  return sample\n",
        "\n",
        "print(seq_to_feature_vec(test_lines_neg[0]))\n",
        "\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVr5hhwBorSv",
        "outputId": "23003cc5-a1c1-48ef-93b2-a360dc77f3d1"
      },
      "execution_count": 261,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  1.38031242 -0.82326774 -0.19253774  0.63351482  0.67304056 -0.13803371\n",
            " -0.81507041  0.11807512 -0.51756084 -0.18004075  0.26749709 -1.03387915\n",
            "  0.10955519 -0.6686416  -0.31646149 -0.7073931   0.40145539 -0.54970748\n",
            " -0.40009056 -0.14803237  0.61644437 -1.93865061 -1.06454781  0.65497863\n",
            "  1.06226006 -0.21778299  0.41469161  0.29006565 -0.40773828 -0.51613059\n",
            " -2.27207946  2.91650908 -0.64357908 -0.7073931   0.40145539  0.99333106\n",
            "  0.64014489 -0.56356183  0.38992938  0.44298405 -0.53667286 -0.28672352\n",
            " -0.81507041  1.19911839  1.41123733  0.48010867  0.11167354 -2.00465769\n",
            "  0.7287802   0.32112393 -0.12812106 -0.5154466  -1.5349765  -0.80688057\n",
            " -0.74016753  1.66990901  0.32521081  0.01428981  0.47508745 -1.60510654\n",
            " -0.17368818  0.95247259 -0.16394784  1.06023998 -0.82326774 -0.19253774\n",
            "  0.82404559  0.32112393 -0.13803371 -0.81507041  0.11807512 -0.7425873\n",
            " -0.90020375  1.66990901 -0.12781917 -1.46232368  0.32112393 -1.65466981\n",
            " -0.0941006   0.11807512 -0.13180121 -0.90020375  1.66990901 -0.12781917\n",
            " -1.46232368  0.32112393 -1.65466981 -0.0941006   0.11807512 -0.13180121\n",
            "  0.48010867  0.11167354 -2.00465769  0.7287802   0.32112393 -0.12812106\n",
            " -0.5154466  -1.5349765  -0.80688057 -0.90020375  1.66990901 -0.12781917\n",
            " -1.46232368  0.32112393 -1.65466981 -0.0941006   0.11807512 -0.13180121\n",
            "  0.48010867  0.11167354 -2.00465769  0.7287802   0.32112393 -0.12812106\n",
            " -0.5154466  -1.5349765  -0.80688057  1.06023998 -0.82326774 -0.19253774\n",
            "  0.82404559  0.32112393 -0.13803371 -0.81507041  0.11807512 -0.7425873\n",
            "  1.06023998 -0.82326774 -0.19253774  0.82404559  0.32112393 -0.13803371\n",
            " -0.81507041  0.11807512 -0.7425873 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sequences_list_to_dataset(input):\n",
        "  result = []\n",
        "  for i in input:\n",
        "    vec = seq_to_feature_vec(i)\n",
        "    result.append(vec)\n",
        "  return np.array(result)"
      ],
      "metadata": {
        "id": "E74dc1CPs7-y"
      },
      "execution_count": 262,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_pos=sequences_list_to_dataset(train_lines_pos)\n",
        "train_neg=sequences_list_to_dataset(train_lines_neg)"
      ],
      "metadata": {
        "id": "kAy64lQi38fd"
      },
      "execution_count": 263,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_pos=sequences_list_to_dataset(test_lines_pos)\n",
        "test_neg=sequences_list_to_dataset(test_lines_neg)"
      ],
      "metadata": {
        "id": "XpJf9Zak5_06"
      },
      "execution_count": 264,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.concatenate([train_pos,train_neg])\n",
        "Y_train = np.concatenate([np.ones(train_pos.shape[0]), np.zeros(train_neg.shape[0])])"
      ],
      "metadata": {
        "id": "1Bxt3-koobb0"
      },
      "execution_count": 265,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_neg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNDeGVpzBKd-",
        "outputId": "24d5fe12-1ee3-4d80-ee4a-b2fd435895ec"
      },
      "execution_count": 266,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3974"
            ]
          },
          "metadata": {},
          "execution_count": 266
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_pos_multiplied = np.tile(test_pos, (5,1))\n",
        "X_test = np.concatenate([test_pos_multiplied,test_neg])\n",
        "Y_test = np.concatenate([np.ones(test_pos_multiplied.shape[0]), np.zeros(test_neg.shape[0])])"
      ],
      "metadata": {
        "id": "gorIwMin5mG9"
      },
      "execution_count": 267,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=100)\n",
        "pca.fit(X_train)\n",
        "X_train_pca = pca.transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)"
      ],
      "metadata": {
        "id": "kkK4qMZTFB3Z"
      },
      "execution_count": 268,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#AdaBoost"
      ],
      "metadata": {
        "id": "oj6vMcOfRDEL"
      },
      "execution_count": 269,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = AdaBoostClassifier(n_estimators = 500)\n",
        "classifier=classifier.fit(X_train_pca, Y_train)"
      ],
      "metadata": {
        "id": "l3B3riUUpcle"
      },
      "execution_count": 284,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = classifier.predict(X_test_pca)\n",
        "y_pred_score = classifier.predict_proba(X_test_pca)\n"
      ],
      "metadata": {
        "id": "4VjMbs0IpdiK"
      },
      "execution_count": 286,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLXsvNHG7EUe",
        "outputId": "94494711-f685-4f7f-b0d2-cdd14c7289fa"
      },
      "execution_count": 287,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
              "       1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
              "       1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
              "       1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
              "       1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
              "       0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
              "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
              "       0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
              "       0., 0., 0., 1., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 287
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlQbr9sM_n4n",
        "outputId": "d9d1035a-8fd8-47af-e13d-e6812371ed09"
      },
      "execution_count": 288,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.49960009, 0.50039991],\n",
              "       [0.50126706, 0.49873294],\n",
              "       [0.50277551, 0.49722449],\n",
              "       ...,\n",
              "       [0.49954775, 0.50045225],\n",
              "       [0.50187624, 0.49812376],\n",
              "       [0.50436912, 0.49563088]])"
            ]
          },
          "metadata": {},
          "execution_count": 288
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#printuje wyniki"
      ],
      "metadata": {
        "id": "8IKcVSMYRKzi"
      },
      "execution_count": 274,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "roc_auc_score(Y_test, y_pred_score[:,1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6Pp0p7z_BcU",
        "outputId": "8217db6a-fe89-48cd-946b-4c50e8439768"
      },
      "execution_count": 289,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.48629292757565445"
            ]
          },
          "metadata": {},
          "execution_count": 289
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = confusion_matrix(Y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(result)\n",
        "result1 = classification_report(Y_test, y_pred)\n",
        "print(\"Classification Report:\",)\n",
        "print (result1)\n",
        "result2 = accuracy_score(Y_test,y_pred)\n",
        "print(\"Accuracy:\",result2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NBg75tUco6A",
        "outputId": "2a2f379a-581b-4a05-92d6-511a1bca142a"
      },
      "execution_count": 290,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[310  36]\n",
            " [310  30]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.50      0.90      0.64       346\n",
            "         1.0       0.45      0.09      0.15       340\n",
            "\n",
            "    accuracy                           0.50       686\n",
            "   macro avg       0.48      0.49      0.39       686\n",
            "weighted avg       0.48      0.50      0.40       686\n",
            "\n",
            "Accuracy: 0.4956268221574344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hIWuvW0L50_f"
      },
      "execution_count": 277,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sum(input):\n",
        "    sum = 0\n",
        "    for i in input:\n",
        "        sum = sum + i\n",
        "    return(sum)"
      ],
      "metadata": {
        "id": "_lsvYTW-DXZs"
      },
      "execution_count": 278,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(pca.explained_variance_ratio_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YYJm3JpDxFD",
        "outputId": "9f6309f1-05d0-4a99-b2a1-2b8f6de1a6f6"
      },
      "execution_count": 279,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8677290148716644"
            ]
          },
          "metadata": {},
          "execution_count": 279
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf = svm.SVC(kernel='rbf')\n",
        "\n",
        "clf.fit(X_train, Y_train)\n",
        "\n",
        "y_pred_svm = clf.predict(X_test)\n"
      ],
      "metadata": {
        "id": "c1J3gxwwFVMA"
      },
      "execution_count": 294,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, BatchNormalization\n",
        "from keras.metrics import AUC\n",
        "#tworzę model\n",
        "model = Sequential()\n",
        "model.add(Dense(32, activation='relu', kernel_regularizer='l1'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(1, activation='sigmoid', kernel_regularizer='l1'))\n",
        "#wywołuję model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', AUC()])\n",
        "#dopasowuje model\n",
        "model.fit(X_train_pca, Y_train, epochs=300, batch_size=32)\n",
        "#sprawdzam model na zbiorze testowym \n",
        "scores = model.evaluate(X_test_pca, Y_test)\n",
        "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVkaeK75J0iv",
        "outputId": "e87cb9a6-113a-41a1-e62b-3d0fb8a37e18"
      },
      "execution_count": 298,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "240/240 [==============================] - 2s 2ms/step - loss: 2.9649 - accuracy: 0.5645 - auc_5: 0.5888\n",
            "Epoch 2/300\n",
            "240/240 [==============================] - 0s 2ms/step - loss: 1.4307 - accuracy: 0.6670 - auc_5: 0.7236\n",
            "Epoch 3/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.9683 - accuracy: 0.7049 - auc_5: 0.7757\n",
            "Epoch 4/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.8414 - accuracy: 0.7266 - auc_5: 0.8004\n",
            "Epoch 5/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.7892 - accuracy: 0.7482 - auc_5: 0.8237\n",
            "Epoch 6/300\n",
            "240/240 [==============================] - 0s 2ms/step - loss: 0.7605 - accuracy: 0.7700 - auc_5: 0.8467\n",
            "Epoch 7/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.7413 - accuracy: 0.7903 - auc_5: 0.8645\n",
            "Epoch 8/300\n",
            "240/240 [==============================] - 0s 2ms/step - loss: 0.7275 - accuracy: 0.8025 - auc_5: 0.8809\n",
            "Epoch 9/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.7171 - accuracy: 0.8168 - auc_5: 0.8932\n",
            "Epoch 10/300\n",
            "240/240 [==============================] - 0s 2ms/step - loss: 0.7067 - accuracy: 0.8310 - auc_5: 0.9030\n",
            "Epoch 11/300\n",
            "240/240 [==============================] - 0s 2ms/step - loss: 0.7077 - accuracy: 0.8297 - auc_5: 0.9054\n",
            "Epoch 12/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6903 - accuracy: 0.8459 - auc_5: 0.9186\n",
            "Epoch 13/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6810 - accuracy: 0.8492 - auc_5: 0.9245\n",
            "Epoch 14/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6806 - accuracy: 0.8492 - auc_5: 0.9257\n",
            "Epoch 15/300\n",
            "240/240 [==============================] - 0s 2ms/step - loss: 0.6796 - accuracy: 0.8494 - auc_5: 0.9284\n",
            "Epoch 16/300\n",
            "240/240 [==============================] - 0s 2ms/step - loss: 0.6611 - accuracy: 0.8676 - auc_5: 0.9382\n",
            "Epoch 17/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6653 - accuracy: 0.8678 - auc_5: 0.9369\n",
            "Epoch 18/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6672 - accuracy: 0.8593 - auc_5: 0.9370\n",
            "Epoch 19/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6692 - accuracy: 0.8635 - auc_5: 0.9359\n",
            "Epoch 20/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6578 - accuracy: 0.8742 - auc_5: 0.9422\n",
            "Epoch 21/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6617 - accuracy: 0.8663 - auc_5: 0.9400\n",
            "Epoch 22/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6565 - accuracy: 0.8755 - auc_5: 0.9437\n",
            "Epoch 23/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6570 - accuracy: 0.8715 - auc_5: 0.9437\n",
            "Epoch 24/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6508 - accuracy: 0.8766 - auc_5: 0.9444\n",
            "Epoch 25/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6457 - accuracy: 0.8824 - auc_5: 0.9475\n",
            "Epoch 26/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6478 - accuracy: 0.8773 - auc_5: 0.9477\n",
            "Epoch 27/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6429 - accuracy: 0.8840 - auc_5: 0.9506\n",
            "Epoch 28/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6410 - accuracy: 0.8836 - auc_5: 0.9505\n",
            "Epoch 29/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6437 - accuracy: 0.8796 - auc_5: 0.9497\n",
            "Epoch 30/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6552 - accuracy: 0.8830 - auc_5: 0.9475\n",
            "Epoch 31/300\n",
            "240/240 [==============================] - 0s 2ms/step - loss: 0.6387 - accuracy: 0.8905 - auc_5: 0.9534\n",
            "Epoch 32/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6403 - accuracy: 0.8864 - auc_5: 0.9514\n",
            "Epoch 33/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6409 - accuracy: 0.8836 - auc_5: 0.9501\n",
            "Epoch 34/300\n",
            "240/240 [==============================] - 0s 2ms/step - loss: 0.6293 - accuracy: 0.8929 - auc_5: 0.9564\n",
            "Epoch 35/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6312 - accuracy: 0.8944 - auc_5: 0.9549\n",
            "Epoch 36/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6367 - accuracy: 0.8882 - auc_5: 0.9525\n",
            "Epoch 37/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6278 - accuracy: 0.8909 - auc_5: 0.9545\n",
            "Epoch 38/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6317 - accuracy: 0.8905 - auc_5: 0.9547\n",
            "Epoch 39/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6365 - accuracy: 0.8926 - auc_5: 0.9519\n",
            "Epoch 40/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6340 - accuracy: 0.8883 - auc_5: 0.9533\n",
            "Epoch 41/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6312 - accuracy: 0.8864 - auc_5: 0.9550\n",
            "Epoch 42/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6317 - accuracy: 0.8887 - auc_5: 0.9541\n",
            "Epoch 43/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6213 - accuracy: 0.8950 - auc_5: 0.9575\n",
            "Epoch 44/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6322 - accuracy: 0.8896 - auc_5: 0.9530\n",
            "Epoch 45/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6262 - accuracy: 0.8897 - auc_5: 0.9562\n",
            "Epoch 46/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6240 - accuracy: 0.8951 - auc_5: 0.9572\n",
            "Epoch 47/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6256 - accuracy: 0.8924 - auc_5: 0.9571\n",
            "Epoch 48/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6353 - accuracy: 0.8901 - auc_5: 0.9525\n",
            "Epoch 49/300\n",
            "240/240 [==============================] - 0s 2ms/step - loss: 0.6189 - accuracy: 0.8959 - auc_5: 0.9580\n",
            "Epoch 50/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6126 - accuracy: 0.8969 - auc_5: 0.9595\n",
            "Epoch 51/300\n",
            "240/240 [==============================] - 0s 2ms/step - loss: 0.6088 - accuracy: 0.8989 - auc_5: 0.9615\n",
            "Epoch 52/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6291 - accuracy: 0.8924 - auc_5: 0.9560\n",
            "Epoch 53/300\n",
            "240/240 [==============================] - 0s 2ms/step - loss: 0.6269 - accuracy: 0.8933 - auc_5: 0.9569\n",
            "Epoch 54/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6074 - accuracy: 0.9029 - auc_5: 0.9625\n",
            "Epoch 55/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6093 - accuracy: 0.8957 - auc_5: 0.9619\n",
            "Epoch 56/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6346 - accuracy: 0.8890 - auc_5: 0.9541\n",
            "Epoch 57/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6066 - accuracy: 0.9006 - auc_5: 0.9627\n",
            "Epoch 58/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6242 - accuracy: 0.8935 - auc_5: 0.9561\n",
            "Epoch 59/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6127 - accuracy: 0.8999 - auc_5: 0.9601\n",
            "Epoch 60/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6229 - accuracy: 0.8917 - auc_5: 0.9559\n",
            "Epoch 61/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6112 - accuracy: 0.8957 - auc_5: 0.9583\n",
            "Epoch 62/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6384 - accuracy: 0.8834 - auc_5: 0.9502\n",
            "Epoch 63/300\n",
            "240/240 [==============================] - 0s 2ms/step - loss: 0.6187 - accuracy: 0.8938 - auc_5: 0.9574\n",
            "Epoch 64/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6165 - accuracy: 0.8935 - auc_5: 0.9575\n",
            "Epoch 65/300\n",
            "240/240 [==============================] - 0s 2ms/step - loss: 0.6282 - accuracy: 0.8886 - auc_5: 0.9553\n",
            "Epoch 66/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6206 - accuracy: 0.8948 - auc_5: 0.9574\n",
            "Epoch 67/300\n",
            "240/240 [==============================] - 0s 2ms/step - loss: 0.6259 - accuracy: 0.8862 - auc_5: 0.9543\n",
            "Epoch 68/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6025 - accuracy: 0.8999 - auc_5: 0.9634\n",
            "Epoch 69/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6087 - accuracy: 0.9024 - auc_5: 0.9614\n",
            "Epoch 70/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6279 - accuracy: 0.8938 - auc_5: 0.9542\n",
            "Epoch 71/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6130 - accuracy: 0.8947 - auc_5: 0.9610\n",
            "Epoch 72/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6030 - accuracy: 0.9034 - auc_5: 0.9626\n",
            "Epoch 73/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6099 - accuracy: 0.8978 - auc_5: 0.9596\n",
            "Epoch 74/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6153 - accuracy: 0.8993 - auc_5: 0.9597\n",
            "Epoch 75/300\n",
            "240/240 [==============================] - 0s 2ms/step - loss: 0.6073 - accuracy: 0.9042 - auc_5: 0.9619\n",
            "Epoch 76/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6066 - accuracy: 0.8989 - auc_5: 0.9612\n",
            "Epoch 77/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6173 - accuracy: 0.8916 - auc_5: 0.9574\n",
            "Epoch 78/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6213 - accuracy: 0.8943 - auc_5: 0.9569\n",
            "Epoch 79/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6093 - accuracy: 0.8963 - auc_5: 0.9597\n",
            "Epoch 80/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6251 - accuracy: 0.8890 - auc_5: 0.9545\n",
            "Epoch 81/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6240 - accuracy: 0.8916 - auc_5: 0.9561\n",
            "Epoch 82/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6238 - accuracy: 0.8929 - auc_5: 0.9571\n",
            "Epoch 83/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6071 - accuracy: 0.9036 - auc_5: 0.9618\n",
            "Epoch 84/300\n",
            "240/240 [==============================] - 0s 2ms/step - loss: 0.6160 - accuracy: 0.8959 - auc_5: 0.9578\n",
            "Epoch 85/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6134 - accuracy: 0.8947 - auc_5: 0.9588\n",
            "Epoch 86/300\n",
            "240/240 [==============================] - 0s 2ms/step - loss: 0.6253 - accuracy: 0.8909 - auc_5: 0.9555\n",
            "Epoch 87/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5917 - accuracy: 0.9080 - auc_5: 0.9662\n",
            "Epoch 88/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6133 - accuracy: 0.8982 - auc_5: 0.9573\n",
            "Epoch 89/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6270 - accuracy: 0.8883 - auc_5: 0.9558\n",
            "Epoch 90/300\n",
            "240/240 [==============================] - 0s 2ms/step - loss: 0.6150 - accuracy: 0.8954 - auc_5: 0.9577\n",
            "Epoch 91/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6164 - accuracy: 0.8960 - auc_5: 0.9574\n",
            "Epoch 92/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6174 - accuracy: 0.8989 - auc_5: 0.9565\n",
            "Epoch 93/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6071 - accuracy: 0.9001 - auc_5: 0.9609\n",
            "Epoch 94/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6215 - accuracy: 0.8909 - auc_5: 0.9547\n",
            "Epoch 95/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6114 - accuracy: 0.8969 - auc_5: 0.9596\n",
            "Epoch 96/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6205 - accuracy: 0.8913 - auc_5: 0.9550\n",
            "Epoch 97/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6314 - accuracy: 0.8832 - auc_5: 0.9524\n",
            "Epoch 98/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6087 - accuracy: 0.8986 - auc_5: 0.9604\n",
            "Epoch 99/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6206 - accuracy: 0.8891 - auc_5: 0.9554\n",
            "Epoch 100/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6207 - accuracy: 0.8909 - auc_5: 0.9555\n",
            "Epoch 101/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6214 - accuracy: 0.8890 - auc_5: 0.9554\n",
            "Epoch 102/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6080 - accuracy: 0.9023 - auc_5: 0.9606\n",
            "Epoch 103/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6120 - accuracy: 0.8969 - auc_5: 0.9592\n",
            "Epoch 104/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6279 - accuracy: 0.8909 - auc_5: 0.9551\n",
            "Epoch 105/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6201 - accuracy: 0.8943 - auc_5: 0.9580\n",
            "Epoch 106/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6078 - accuracy: 0.9008 - auc_5: 0.9613\n",
            "Epoch 107/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6232 - accuracy: 0.8914 - auc_5: 0.9542\n",
            "Epoch 108/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6237 - accuracy: 0.8873 - auc_5: 0.9535\n",
            "Epoch 109/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6064 - accuracy: 0.8997 - auc_5: 0.9588\n",
            "Epoch 110/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6212 - accuracy: 0.8904 - auc_5: 0.9561\n",
            "Epoch 111/300\n",
            "240/240 [==============================] - 0s 2ms/step - loss: 0.6136 - accuracy: 0.8950 - auc_5: 0.9570\n",
            "Epoch 112/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6116 - accuracy: 0.8935 - auc_5: 0.9566\n",
            "Epoch 113/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6098 - accuracy: 0.8934 - auc_5: 0.9578\n",
            "Epoch 114/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6240 - accuracy: 0.8887 - auc_5: 0.9539\n",
            "Epoch 115/300\n",
            "240/240 [==============================] - 0s 2ms/step - loss: 0.6126 - accuracy: 0.8978 - auc_5: 0.9592\n",
            "Epoch 116/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6140 - accuracy: 0.8957 - auc_5: 0.9578\n",
            "Epoch 117/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6286 - accuracy: 0.8916 - auc_5: 0.9529\n",
            "Epoch 118/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6061 - accuracy: 0.8997 - auc_5: 0.9612\n",
            "Epoch 119/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5969 - accuracy: 0.9042 - auc_5: 0.9641\n",
            "Epoch 120/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6031 - accuracy: 0.9010 - auc_5: 0.9602\n",
            "Epoch 121/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6175 - accuracy: 0.8954 - auc_5: 0.9573\n",
            "Epoch 122/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6313 - accuracy: 0.8908 - auc_5: 0.9527\n",
            "Epoch 123/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6137 - accuracy: 0.8965 - auc_5: 0.9589\n",
            "Epoch 124/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5950 - accuracy: 0.9093 - auc_5: 0.9637\n",
            "Epoch 125/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6262 - accuracy: 0.8914 - auc_5: 0.9540\n",
            "Epoch 126/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6163 - accuracy: 0.8904 - auc_5: 0.9534\n",
            "Epoch 127/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6275 - accuracy: 0.8822 - auc_5: 0.9512\n",
            "Epoch 128/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6109 - accuracy: 0.8937 - auc_5: 0.9562\n",
            "Epoch 129/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6157 - accuracy: 0.8900 - auc_5: 0.9557\n",
            "Epoch 130/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6160 - accuracy: 0.8917 - auc_5: 0.9562\n",
            "Epoch 131/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6094 - accuracy: 0.8942 - auc_5: 0.9577\n",
            "Epoch 132/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6013 - accuracy: 0.8980 - auc_5: 0.9600\n",
            "Epoch 133/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6134 - accuracy: 0.8954 - auc_5: 0.9562\n",
            "Epoch 134/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6084 - accuracy: 0.9006 - auc_5: 0.9573\n",
            "Epoch 135/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6203 - accuracy: 0.8912 - auc_5: 0.9539\n",
            "Epoch 136/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6060 - accuracy: 0.8933 - auc_5: 0.9577\n",
            "Epoch 137/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6091 - accuracy: 0.8965 - auc_5: 0.9568\n",
            "Epoch 138/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6093 - accuracy: 0.8959 - auc_5: 0.9592\n",
            "Epoch 139/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6275 - accuracy: 0.8890 - auc_5: 0.9515\n",
            "Epoch 140/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6256 - accuracy: 0.8930 - auc_5: 0.9558\n",
            "Epoch 141/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6038 - accuracy: 0.8994 - auc_5: 0.9597\n",
            "Epoch 142/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6165 - accuracy: 0.8907 - auc_5: 0.9549\n",
            "Epoch 143/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6127 - accuracy: 0.8946 - auc_5: 0.9563\n",
            "Epoch 144/300\n",
            "240/240 [==============================] - 0s 2ms/step - loss: 0.6023 - accuracy: 0.8916 - auc_5: 0.9601\n",
            "Epoch 145/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6244 - accuracy: 0.8916 - auc_5: 0.9535\n",
            "Epoch 146/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6181 - accuracy: 0.8959 - auc_5: 0.9571\n",
            "Epoch 147/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6120 - accuracy: 0.8969 - auc_5: 0.9587\n",
            "Epoch 148/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6125 - accuracy: 0.8944 - auc_5: 0.9583\n",
            "Epoch 149/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6087 - accuracy: 0.8991 - auc_5: 0.9610\n",
            "Epoch 150/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5955 - accuracy: 0.9040 - auc_5: 0.9627\n",
            "Epoch 151/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6138 - accuracy: 0.8942 - auc_5: 0.9573\n",
            "Epoch 152/300\n",
            "240/240 [==============================] - 0s 2ms/step - loss: 0.6289 - accuracy: 0.8901 - auc_5: 0.9523\n",
            "Epoch 153/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6106 - accuracy: 0.8981 - auc_5: 0.9578\n",
            "Epoch 154/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6049 - accuracy: 0.8967 - auc_5: 0.9585\n",
            "Epoch 155/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5980 - accuracy: 0.9003 - auc_5: 0.9610\n",
            "Epoch 156/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6061 - accuracy: 0.8950 - auc_5: 0.9598\n",
            "Epoch 157/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6046 - accuracy: 0.9032 - auc_5: 0.9603\n",
            "Epoch 158/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6188 - accuracy: 0.8963 - auc_5: 0.9556\n",
            "Epoch 159/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6185 - accuracy: 0.8925 - auc_5: 0.9553\n",
            "Epoch 160/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6185 - accuracy: 0.8899 - auc_5: 0.9556\n",
            "Epoch 161/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5953 - accuracy: 0.9002 - auc_5: 0.9608\n",
            "Epoch 162/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6080 - accuracy: 0.8938 - auc_5: 0.9566\n",
            "Epoch 163/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6004 - accuracy: 0.8977 - auc_5: 0.9584\n",
            "Epoch 164/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6009 - accuracy: 0.8993 - auc_5: 0.9594\n",
            "Epoch 165/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6207 - accuracy: 0.8916 - auc_5: 0.9529\n",
            "Epoch 166/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5938 - accuracy: 0.9015 - auc_5: 0.9603\n",
            "Epoch 167/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6124 - accuracy: 0.8890 - auc_5: 0.9547\n",
            "Epoch 168/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6226 - accuracy: 0.8873 - auc_5: 0.9514\n",
            "Epoch 169/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5995 - accuracy: 0.8964 - auc_5: 0.9593\n",
            "Epoch 170/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6021 - accuracy: 0.8999 - auc_5: 0.9602\n",
            "Epoch 171/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6133 - accuracy: 0.8903 - auc_5: 0.9559\n",
            "Epoch 172/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6031 - accuracy: 0.9002 - auc_5: 0.9600\n",
            "Epoch 173/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6134 - accuracy: 0.8909 - auc_5: 0.9560\n",
            "Epoch 174/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5991 - accuracy: 0.8982 - auc_5: 0.9587\n",
            "Epoch 175/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6021 - accuracy: 0.8998 - auc_5: 0.9595\n",
            "Epoch 176/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6122 - accuracy: 0.8950 - auc_5: 0.9556\n",
            "Epoch 177/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6084 - accuracy: 0.8991 - auc_5: 0.9569\n",
            "Epoch 178/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5869 - accuracy: 0.9006 - auc_5: 0.9626\n",
            "Epoch 179/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6164 - accuracy: 0.8836 - auc_5: 0.9532\n",
            "Epoch 180/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5977 - accuracy: 0.8963 - auc_5: 0.9603\n",
            "Epoch 181/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6105 - accuracy: 0.8920 - auc_5: 0.9570\n",
            "Epoch 182/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6025 - accuracy: 0.8939 - auc_5: 0.9576\n",
            "Epoch 183/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6148 - accuracy: 0.8901 - auc_5: 0.9549\n",
            "Epoch 184/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6082 - accuracy: 0.8941 - auc_5: 0.9556\n",
            "Epoch 185/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6245 - accuracy: 0.8892 - auc_5: 0.9538\n",
            "Epoch 186/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6047 - accuracy: 0.9001 - auc_5: 0.9585\n",
            "Epoch 187/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6076 - accuracy: 0.8942 - auc_5: 0.9573\n",
            "Epoch 188/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5930 - accuracy: 0.9020 - auc_5: 0.9609\n",
            "Epoch 189/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6214 - accuracy: 0.8931 - auc_5: 0.9541\n",
            "Epoch 190/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5990 - accuracy: 0.8987 - auc_5: 0.9604\n",
            "Epoch 191/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6051 - accuracy: 0.8959 - auc_5: 0.9581\n",
            "Epoch 192/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6101 - accuracy: 0.8927 - auc_5: 0.9559\n",
            "Epoch 193/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6214 - accuracy: 0.8904 - auc_5: 0.9530\n",
            "Epoch 194/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6033 - accuracy: 0.8985 - auc_5: 0.9604\n",
            "Epoch 195/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6105 - accuracy: 0.8943 - auc_5: 0.9565\n",
            "Epoch 196/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6052 - accuracy: 0.8965 - auc_5: 0.9588\n",
            "Epoch 197/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6290 - accuracy: 0.8896 - auc_5: 0.9544\n",
            "Epoch 198/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5912 - accuracy: 0.9088 - auc_5: 0.9632\n",
            "Epoch 199/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6129 - accuracy: 0.8959 - auc_5: 0.9566\n",
            "Epoch 200/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6116 - accuracy: 0.8972 - auc_5: 0.9568\n",
            "Epoch 201/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6034 - accuracy: 0.8964 - auc_5: 0.9594\n",
            "Epoch 202/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6045 - accuracy: 0.8971 - auc_5: 0.9585\n",
            "Epoch 203/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5890 - accuracy: 0.9004 - auc_5: 0.9617\n",
            "Epoch 204/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6127 - accuracy: 0.8950 - auc_5: 0.9543\n",
            "Epoch 205/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6160 - accuracy: 0.8896 - auc_5: 0.9520\n",
            "Epoch 206/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6050 - accuracy: 0.8946 - auc_5: 0.9580\n",
            "Epoch 207/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6023 - accuracy: 0.8920 - auc_5: 0.9566\n",
            "Epoch 208/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6013 - accuracy: 0.8924 - auc_5: 0.9558\n",
            "Epoch 209/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5942 - accuracy: 0.8942 - auc_5: 0.9589\n",
            "Epoch 210/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6122 - accuracy: 0.8918 - auc_5: 0.9540\n",
            "Epoch 211/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5943 - accuracy: 0.9028 - auc_5: 0.9611\n",
            "Epoch 212/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6128 - accuracy: 0.8933 - auc_5: 0.9550\n",
            "Epoch 213/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6228 - accuracy: 0.8937 - auc_5: 0.9550\n",
            "Epoch 214/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5846 - accuracy: 0.9091 - auc_5: 0.9637\n",
            "Epoch 215/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6000 - accuracy: 0.8947 - auc_5: 0.9590\n",
            "Epoch 216/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5949 - accuracy: 0.8974 - auc_5: 0.9604\n",
            "Epoch 217/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6160 - accuracy: 0.8891 - auc_5: 0.9518\n",
            "Epoch 218/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6146 - accuracy: 0.8896 - auc_5: 0.9529\n",
            "Epoch 219/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6055 - accuracy: 0.8934 - auc_5: 0.9539\n",
            "Epoch 220/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5821 - accuracy: 0.8993 - auc_5: 0.9626\n",
            "Epoch 221/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6178 - accuracy: 0.8880 - auc_5: 0.9530\n",
            "Epoch 222/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6174 - accuracy: 0.8864 - auc_5: 0.9499\n",
            "Epoch 223/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5945 - accuracy: 0.8955 - auc_5: 0.9575\n",
            "Epoch 224/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6077 - accuracy: 0.8905 - auc_5: 0.9521\n",
            "Epoch 225/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5936 - accuracy: 0.8973 - auc_5: 0.9584\n",
            "Epoch 226/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5968 - accuracy: 0.8929 - auc_5: 0.9583\n",
            "Epoch 227/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5958 - accuracy: 0.8978 - auc_5: 0.9585\n",
            "Epoch 228/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5928 - accuracy: 0.8997 - auc_5: 0.9582\n",
            "Epoch 229/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6008 - accuracy: 0.8925 - auc_5: 0.9566\n",
            "Epoch 230/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6093 - accuracy: 0.8955 - auc_5: 0.9555\n",
            "Epoch 231/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5989 - accuracy: 0.8935 - auc_5: 0.9566\n",
            "Epoch 232/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6110 - accuracy: 0.8937 - auc_5: 0.9545\n",
            "Epoch 233/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6179 - accuracy: 0.8877 - auc_5: 0.9535\n",
            "Epoch 234/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5858 - accuracy: 0.9019 - auc_5: 0.9622\n",
            "Epoch 235/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5842 - accuracy: 0.8999 - auc_5: 0.9600\n",
            "Epoch 236/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6042 - accuracy: 0.8939 - auc_5: 0.9569\n",
            "Epoch 237/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6034 - accuracy: 0.8950 - auc_5: 0.9574\n",
            "Epoch 238/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6111 - accuracy: 0.8926 - auc_5: 0.9549\n",
            "Epoch 239/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6041 - accuracy: 0.8984 - auc_5: 0.9583\n",
            "Epoch 240/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5918 - accuracy: 0.9023 - auc_5: 0.9599\n",
            "Epoch 241/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5965 - accuracy: 0.9027 - auc_5: 0.9584\n",
            "Epoch 242/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5808 - accuracy: 0.9049 - auc_5: 0.9630\n",
            "Epoch 243/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5957 - accuracy: 0.8968 - auc_5: 0.9576\n",
            "Epoch 244/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6031 - accuracy: 0.8944 - auc_5: 0.9535\n",
            "Epoch 245/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6008 - accuracy: 0.8934 - auc_5: 0.9556\n",
            "Epoch 246/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6106 - accuracy: 0.8935 - auc_5: 0.9544\n",
            "Epoch 247/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5946 - accuracy: 0.8978 - auc_5: 0.9572\n",
            "Epoch 248/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6001 - accuracy: 0.8934 - auc_5: 0.9560\n",
            "Epoch 249/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5941 - accuracy: 0.8954 - auc_5: 0.9563\n",
            "Epoch 250/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5853 - accuracy: 0.9027 - auc_5: 0.9593\n",
            "Epoch 251/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5960 - accuracy: 0.8951 - auc_5: 0.9589\n",
            "Epoch 252/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5872 - accuracy: 0.9027 - auc_5: 0.9613\n",
            "Epoch 253/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5928 - accuracy: 0.8929 - auc_5: 0.9573\n",
            "Epoch 254/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5963 - accuracy: 0.8971 - auc_5: 0.9575\n",
            "Epoch 255/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6026 - accuracy: 0.8956 - auc_5: 0.9548\n",
            "Epoch 256/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5921 - accuracy: 0.8989 - auc_5: 0.9581\n",
            "Epoch 257/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6059 - accuracy: 0.8890 - auc_5: 0.9530\n",
            "Epoch 258/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6073 - accuracy: 0.8939 - auc_5: 0.9518\n",
            "Epoch 259/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6096 - accuracy: 0.8907 - auc_5: 0.9530\n",
            "Epoch 260/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5877 - accuracy: 0.9015 - auc_5: 0.9608\n",
            "Epoch 261/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6194 - accuracy: 0.8858 - auc_5: 0.9504\n",
            "Epoch 262/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5833 - accuracy: 0.9037 - auc_5: 0.9624\n",
            "Epoch 263/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6006 - accuracy: 0.8952 - auc_5: 0.9568\n",
            "Epoch 264/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5906 - accuracy: 0.9032 - auc_5: 0.9572\n",
            "Epoch 265/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6098 - accuracy: 0.8880 - auc_5: 0.9520\n",
            "Epoch 266/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5985 - accuracy: 0.8938 - auc_5: 0.9556\n",
            "Epoch 267/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5995 - accuracy: 0.8937 - auc_5: 0.9566\n",
            "Epoch 268/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6018 - accuracy: 0.8967 - auc_5: 0.9562\n",
            "Epoch 269/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5951 - accuracy: 0.8955 - auc_5: 0.9588\n",
            "Epoch 270/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6052 - accuracy: 0.8955 - auc_5: 0.9565\n",
            "Epoch 271/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5931 - accuracy: 0.8956 - auc_5: 0.9595\n",
            "Epoch 272/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6007 - accuracy: 0.8934 - auc_5: 0.9558\n",
            "Epoch 273/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6101 - accuracy: 0.8908 - auc_5: 0.9512\n",
            "Epoch 274/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6212 - accuracy: 0.8865 - auc_5: 0.9505\n",
            "Epoch 275/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6046 - accuracy: 0.8922 - auc_5: 0.9555\n",
            "Epoch 276/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6140 - accuracy: 0.8837 - auc_5: 0.9523\n",
            "Epoch 277/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6137 - accuracy: 0.8873 - auc_5: 0.9516\n",
            "Epoch 278/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5842 - accuracy: 0.9019 - auc_5: 0.9607\n",
            "Epoch 279/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6049 - accuracy: 0.8929 - auc_5: 0.9573\n",
            "Epoch 280/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6146 - accuracy: 0.8874 - auc_5: 0.9523\n",
            "Epoch 281/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6070 - accuracy: 0.8950 - auc_5: 0.9557\n",
            "Epoch 282/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6024 - accuracy: 0.8981 - auc_5: 0.9574\n",
            "Epoch 283/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6193 - accuracy: 0.8908 - auc_5: 0.9538\n",
            "Epoch 284/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5932 - accuracy: 0.8950 - auc_5: 0.9567\n",
            "Epoch 285/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6115 - accuracy: 0.8862 - auc_5: 0.9519\n",
            "Epoch 286/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5868 - accuracy: 0.8965 - auc_5: 0.9574\n",
            "Epoch 287/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6032 - accuracy: 0.8926 - auc_5: 0.9530\n",
            "Epoch 288/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5767 - accuracy: 0.9067 - auc_5: 0.9628\n",
            "Epoch 289/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6038 - accuracy: 0.8924 - auc_5: 0.9521\n",
            "Epoch 290/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6057 - accuracy: 0.8931 - auc_5: 0.9540\n",
            "Epoch 291/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6102 - accuracy: 0.8897 - auc_5: 0.9535\n",
            "Epoch 292/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5963 - accuracy: 0.8908 - auc_5: 0.9550\n",
            "Epoch 293/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6052 - accuracy: 0.8912 - auc_5: 0.9525\n",
            "Epoch 294/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5940 - accuracy: 0.8998 - auc_5: 0.9552\n",
            "Epoch 295/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5932 - accuracy: 0.8980 - auc_5: 0.9560\n",
            "Epoch 296/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5925 - accuracy: 0.8944 - auc_5: 0.9550\n",
            "Epoch 297/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6063 - accuracy: 0.8912 - auc_5: 0.9530\n",
            "Epoch 298/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5964 - accuracy: 0.8969 - auc_5: 0.9553\n",
            "Epoch 299/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.6203 - accuracy: 0.8918 - auc_5: 0.9493\n",
            "Epoch 300/300\n",
            "240/240 [==============================] - 1s 2ms/step - loss: 0.5915 - accuracy: 0.8917 - auc_5: 0.9563\n",
            "22/22 [==============================] - 0s 2ms/step - loss: 2.5872 - accuracy: 0.4665 - auc_5: 0.4651\n",
            "\n",
            "accuracy: 46.65%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2vfLrb1xK4xi"
      },
      "execution_count": 282,
      "outputs": []
    }
  ]
}